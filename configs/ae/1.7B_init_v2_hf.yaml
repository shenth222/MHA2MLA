TrainingArguments:
  # general
  seed: 42
  max_steps: 2000
  report_to: wandb
  run_name: 1.7B_rope_v4_topk4_ae_v2_rank8_silu_init_lr1e-3
  save_strategy : steps
  save_steps: 0.5
  output_dir: ../checkpoints/1.7B_rope_v4_topk4_ae_v2_rank8_silu_init
  overwrite_output_dir: true
  logging_strategy: steps
  logging_steps: 10
  resume_from_checkpoint: null
  per_device_train_batch_size: 16
  remove_unused_columns: False
  gradient_accumulation_steps: 1
  bf16: true
  dataloader_drop_last: true
  # optim
  # max_grad_norm: 1.0
  # learning_rate: 1.0e-3
  # weight_decay: 0.01
  # adam_beta1: 0.9
  # adam_beta2: 0.999
  # adam_epsilon: 1.0e-8
  # max_grad_norm: 1.0
  # lr_scheduler_type: "constant"
Optimizer:
  learning_rate_scheduler:
    learning_rate: 1.0e-3 # 学习率
    lr_decay_starting_step: 1000 # 学习率衰减开始步数
    lr_decay_steps: 1000 # 学习率衰减步数
    lr_decay_style: 1-sqrt # 学习率衰减风格
    lr_warmup_steps: 200 # 学习率预热步数
    lr_warmup_style: linear # 学习率预热风格
    min_decay_lr: 0 # 最小衰减学习率
  optimizer_factory:
    adam_beta1: 0.9 # Adam优化器的beta1参数
    adam_beta2: 0.95 # Adam优化器的beta2参数
    adam_eps: 1.0e-08 # Adam优化器的epsilon
    name: adamW # 优化器名称
    torch_adam_is_fused: true # 是否使用Torch的融合Adam
  weight_decay: 0.01 # 权重衰减


# model
ModelArguments:
  dtype: bfloat16
  model_name_or_path: ../checkpoints/HuggingFaceTB/SmolLM-1.7B
  tokenizer_name_or_path: ../checkpoints/HuggingFaceTB/SmolLM-1.7B

# dataset
DataArguments:
  dataset_folders: # 数据集的路径列表
    - ../../../data/smollm1_corpus/fineweb-edu-dedup
    - ../../../data/smollm1_corpus/cosmopedia-v2
    - ../../../data/smollm1_corpus/python-edu
    - ../../../data/smollm1_corpus/open-web-math
    - ../../../data/smollm1_corpus/stackoverflow
  dataset_weights: # 各数据集的权重
    - 0.7
    - 0.15
    - 0.06
    - 0.08
    - 0.01
  sequence_length: 2048
  DP: 4

# model config
ModelConfig:
  attention_bias: false
  attention_dropout: 0.0
  bos_token_id: 0
  eos_token_id: 0
  hidden_act: silu
  hidden_size: 2048
  initializer_range: 0.02
  intermediate_size: 8192
  max_position_embeddings: 2048
  mlp_bias: false
  model_type: llama
  num_attention_heads: 32
  num_hidden_layers: 24
  num_key_value_heads: 32
  pretraining_tp: 1
  rms_norm_eps: 1e-05
  rope_scaling: null
  rope_theta: 10000.0
  tie_word_embeddings: true
  torch_dtype: bfloat16
  use_cache: true
  vocab_size: 49152
  RoPE:
    partial_rope_version: 4  # 0 1 2 3 4 5
    top_k_rope_dim: 4
    last_k_rope_dim: 0
    uniform_start_point: 0
    uniform_step: 1
    qk_tensor_path: '../utils/qk_tensor_1.7B.pth'
    n_gqa_group: 1
  AE:
    version: 2
    low_rank: 8
    activation_fn: silu

# 关注
# run_name
# output_dir
# global_batch_size = DP * per_device_train_batch_size * gradient_accumulation_steps = 64
