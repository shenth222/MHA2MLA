#checkpoints:
  checkpoint_interval: 1800 # 保存检查点的步数间隔
  checkpoints_path: ../checkpoints/1 # 检查点保存的路径
  checkpoints_path_is_shared_file_system: false # 检查点路径是否在共享文件系统上
  resume_checkpoint_path: ~/data/models/HuggingFaceTB/SmolLM-135M_nt # 恢复训练时的检查点路径
  save_final_state: false # 是否保存最终状态
  save_initial_state: false # 是否保存初始状态
  load_lr_scheduler: false
  load_optimizer: false

#data_stages:
  data:
    dataset:
      dataset_folder: # 数据集的路径列表
        - ~/local/smollm1_corpus/fineweb-edu-dedup/
        - ~/local/smollm1_corpus/cosmopedia-v2/
        - ~/local/smollm1_corpus/python-edu/
        - ~/local/smollm1_corpus/open-web-math/
        - ~/local/smollm1_corpus/stackoverflow/
      dataset_weights: # 各数据集的权重
        - 0.7
        - 0.15
        - 0.08
        - 0.06
        - 0.01
    num_loading_workers: 4 # 数据加载的工作线程数
    seed: 42 # 随机种子
  name: training stage # 数据阶段名称
  start_training_step: 1 # 开始训练的步数

#general:
  benchmark_csv_path: null # 基准测试CSV文件路径
  consumed_train_samples: null # 已消耗的训练样本数
  ignore_sanity_checks: true # 是否忽略完整性检查
  project: smollm # 项目名称
  run: smollm-135M_1 # 运行名称
  seed: 42 # 随机种子
  step: null # 当前步数

#logging:
  iteration_step_info_interval: 1 # 每步记录日志信息的间隔
  log_level: info # 日志级别
  log_level_replica: info # 副本的日志级别

#model:
  ddp_bucket_cap_mb: 25 # DDP的bucket大小
  dtype: bfloat16 # 数据类型
  init_method:
    std: 0.0416 # 初始化方法的标准差
  make_vocab_size_divisible_by: 1 # 词汇表大小是否可整除
  model_config:
    bos_token_id: 0 # 序列开始标记的ID
    eos_token_id: 0 # 序列结束标记的ID
    hidden_act: silu # 隐藏层激活函数
    hidden_size: 576 # 隐藏层大小
    initializer_range: 0.02 # 初始化范围
    intermediate_size: 1536 # 中间层大小
    is_llama_config: true # 是否为LLaMA配置
    max_position_embeddings: 2048 # 最大位置嵌入
    num_attention_heads: 9 # 注意力头的数量
    num_hidden_layers: 30 # 隐藏层的数量
    num_key_value_heads: 3 # 键值头的数量
    pad_token_id: null # 填充标记的ID
    pretraining_tp: 1 # 预训练的TP(张量并行度)
    rms_norm_eps: 1.0e-05 # RMSNorm的epsilon
    rope_scaling: null # ROPE缩放
    rope_theta: 10000.0 # ROPE的theta值
    tie_word_embeddings: true # 是否绑定词嵌入
    use_cache: true # 是否使用缓存
    vocab_size: 49152 # 词汇表大小

#optimizer:
  accumulate_grad_in_fp32: true # 是否在FP32中累积梯度
  clip_grad: 1.0 # 梯度裁剪值
  learning_rate_scheduler:
    learning_rate: 0.00001 # 学习率
    lr_decay_starting_step: 900 # 学习率衰减开始步数
    lr_decay_steps: null # 学习率衰减步数
    lr_decay_style: 1-sqrt # 学习率衰减风格
    lr_warmup_steps: 900 # 学习率预热步数
    lr_warmup_style: linear # 学习率预热风格
    min_decay_lr: 0 # 最小衰减学习率
  optimizer_factory:
    adam_beta1: 0.9 # Adam优化器的beta1参数
    adam_beta2: 0.95 # Adam优化器的beta2参数
    adam_eps: 1.0e-08 # Adam优化器的epsilon
    name: adamW # 优化器名称
    torch_adam_is_fused: true # 是否使用Torch的融合Adam
  weight_decay: 0.01 # 权重衰减
  zero_stage: 0 # ZeRO优化的阶段

#parallelism:
  dp: 1 # 数据并行度(并行设备数目)
  expert_parallel_size: 1 # 专家并行大小
  pp: 1 # 管道并行度
  pp_engine: 1f1b # 管道引擎
  recompute_layer: false # 是否重新计算层
  tp: 1 # 张量并行度
  tp_linear_async_communication: true # 张量并行异步通信
  tp_mode: REDUCE_SCATTER # 张量并行模式
  tp_recompute_allgather: true # 张量并行重新计算AllGather

#profiler: null # 性能分析器

#tokenizer:
  tokenizer_max_length: null # 分词器最大长度
  tokenizer_name_or_path: ~/data/models/HuggingFaceTB/SmolLM-135M # 分词器名称或路径
  tokenizer_revision: null # 分词器版本

#tokens:
  batch_accumulation_per_replica: 1 # 每个副本（设备）的批次累积
  limit_test_batches: 0 # 测试批次限制
  limit_val_batches: 0 # 验证批次限制
  micro_batch_size: 16 # 微批次大小（单个设备上样本大小
  sequence_length: 2048 # 序列长度
  train_steps: 18000 # 训练步骤数
  val_check_interval: -1 # 验证检查间隔

  output_dir: .

# pre_train_steps: 600000
# GBS = micro_batch_size *  batch_accumulation_per_replica * dp
# number of tokens = micro_batch_size *  batch_accumulation_per_replica * dp * sequence_length
# 关注：
# checkpoint_interval
# checkpoints_path
# run
# train_steps
# learning_rate
# lr_warmup_steps
# lr_decay_starting_step